---
layout: distill
title: Lecture 08
description: (Multinomial) Logistic Regression 
date: 2025-09-29

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name: Emily Hoffman 
  - name: Author 2
  - name: Author 3

editors:
  - name: Editor 1 # editor's full name

## 4. Logits and Cross-Entropy
## About the term "Logits"
- logits = log-odds unit
- logit(p) = log(p/1-p)
- typically means the net input of the last neuron layer 
- in logistic regression, logits are: w bold^T x

## About the term "Binary Cross Entropy"
- negative log-likelihood and binary cross entropy are equivalent
- need both formulas

## 5. Logistic Regression Code Example 
- https://github.com/rasbt/stat453-deep-learning-ss21/blob/master/L08/code/logistic-regression.ipynb

## 6. Generalizing to Multiple Classes: Softmax Regression 
## Approaches to multi-class classification 
- one-vs-all: predict each class label independently then choose the class with the highest confidence score
- all-vs-all: explicitly predict the probability of each competing outcome then choose the class with the highest confidence score
- predict probabilities of class membership simultaneously
